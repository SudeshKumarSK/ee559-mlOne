{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework : 3 Machine Learning - 1 (Supervised Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code up a 2-class perceptron learning algorithm and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#Importing the nearestMeansClassifier library from utils package.\n",
    "from utils import  perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training Data and Testing Data from dataset-1 using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv(\"./HW3_datasets/dataset1_train.csv\", header=None)\n",
    "print(\"Train Data -> 1: \")\n",
    "print(train_data_1.head())\n",
    "print()\n",
    "\n",
    "test_data_1 = pd.read_csv(\"./HW3_datasets/dataset1_test.csv\", header=None)\n",
    "print(\"Test Data -> 1: \")\n",
    "print(test_data_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training Data and Testing Data from dataset-2 using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = pd.read_csv(\"./HW3_datasets/dataset2_train.csv\", header=None)\n",
    "print(\"Train Data -> 2: \")\n",
    "print(train_data_2.head())\n",
    "print()\n",
    "\n",
    "test_data_2 = pd.read_csv(\"./HW3_datasets/dataset2_test.csv\", header=None)\n",
    "print(\"Train Data -> 2: \")\n",
    "print(test_data_2.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training Data and Testing Data from dataset-3 using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3 = pd.read_csv(\"./HW3_datasets/dataset3_train.csv\", header=None)\n",
    "print(\"Train Data -> 3: \")\n",
    "print(train_data_3.head())\n",
    "print()\n",
    "\n",
    "test_data_3 = pd.read_csv(\"./HW3_datasets/dataset3_test.csv\", header=None)\n",
    "print(\"Train Data -> 3: \")\n",
    "print(test_data_3.head())\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (a) Perform the following for Dataset - 1 of Homework -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Run the perceptron learning algorithm to find optimum w_vector using Sequential Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_1 = perceptron.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_1, X_train_1, T_train_1 = perceptron_1.generateTrainData(trainData=train_data_1)\n",
    "X_train_augmented_1 = perceptron_1.augmentData(X=X_train_1, n=n_train_1)\n",
    "print(f\"Shape of Augmented X_train_1: {X_train_augmented_1.shape}\")\n",
    "\n",
    "T_train_changed_1 = perceptron_1.changeLabels(T = T_train_1)\n",
    "\n",
    "X_train_shuffled_1, T_train_shuffled_1 = perceptron_1.shuffleData(X=X_train_augmented_1, T=T_train_changed_1.reshape(n_train_1, 1))\n",
    "\n",
    "w_vector_1 = perceptron_1.initializeWeights(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergenceFlag_1, n_epochs_1, n_epochs_arr_1, n_iters_1, n_iters_arr_1, J_History_epochs_1, w_History_epochs_1, J_History_iterations_1, cer_History_epochs_1, cer_History_iterations_1 = perceptron_1.modelTrain_SequentialGD(\n",
    "                                                                                                                                                                n_train = n_train_1, \n",
    "                                                                                                                                                                X_train=X_train_shuffled_1, \n",
    "                                                                                                                                                                T_train=T_train_shuffled_1, \n",
    "                                                                                                                                                                w_vector=w_vector_1, \n",
    "                                                                                                                                                                epochs=100, \n",
    "                                                                                                                                                                learn_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Give the resulting optimum w vector; state whether the algorithm converged (i.1 reached) or halted without convergence (i.2 reached); and give the final criterion function value J(w_optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_index_epochs_1 = np.argmin(J_History_epochs_1)\n",
    "optimum_index_iterations_1 = np.argmin(cer_History_iterations_1)\n",
    "\n",
    "w_optimum_epochs_1 = w_History_epochs_1[optimum_index_epochs_1]\n",
    "J_optimum_epochs_1 = J_History_epochs_1[optimum_index_epochs_1]\n",
    "J_optimum_iterations_1 = J_History_iterations_1[optimum_index_iterations_1]\n",
    "cer_optimum_epochs_1 = cer_History_epochs_1[optimum_index_epochs_1]\n",
    "cer_optimum_iterations_1 = cer_History_iterations_1[optimum_index_iterations_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convergenceFlag_1:\n",
    "    print(f\"The Perceptron Learning Algorithm got converged after {n_iters_1} iterations and {n_epochs_1} epoch(s) for Training set of Dataset - 1!\")\n",
    "    print(\"DATASET - 1 IS LINEARLY SEPARABLE!!\")\n",
    "else:\n",
    "    print(f\"The Perceptron Learning Algorithm did not converge for Training set of Dataset - 1 and it took {n_iters_1} iterations and {n_epochs_1} epoch(s)!\")\n",
    "\n",
    "print(f\"The optimum value of w_vector for Training Set of Dataset - 1 is: {w_optimum_epochs_1}\")\n",
    "print(f\"The Final Criterion function value for optimum value of w is: {J_optimum_epochs_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Produce a learning curve, which is a plot of the values of the criterion function during the training process. If the training goes for more than 10 epochs, plot the criterion function vs. epochs. If training ends before 10 epochs, plot the criterion function vs. iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_1 <= 10:\n",
    "    perceptron_1.plotCriterionVsIters(n_iters=n_iters_arr_1, J_History_iters=J_History_iterations_1, J_optimum_iters=J_optimum_iterations_1, datasetName=\"Training Data Dataset - 1\")\n",
    "\n",
    "else:\n",
    "    perceptron_1.plotCriterionVsEpochs(n_epochs=n_epochs_arr_1, J_History_epochs=J_History_epochs_1, J_optimum_epochs=J_optimum_epochs_1, datasetName=\"Training Data Dataset - 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Plot of Classification Error Rate (CER) Vs. Number of Epochs (or) Plot of Classification Error Rate (CER) Vs. Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_1 <= 10:\n",
    "    perceptron_1.plotCERVsIters(n_iters=n_iters_arr_1, cer_History_iters=cer_History_iterations_1, cer_optimum_iters=cer_optimum_iterations_1, datasetName=\"Training Data Dataset - 1\")\n",
    "\n",
    "else:\n",
    "    perceptron_1.plotCriterionVsEpochs(n_epochs=n_epochs_arr_1, cer_History_epochs=cer_History_epochs_1, cer_optimum_epochs=cer_optimum_epochs_1, datasetName=\"Training Data Dataset - 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the training set using the optimum w_vector . Give the classification error of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_training_1 = perceptron_1.predict(X=X_train_augmented_1, w_optimum=w_optimum_epochs_1)\n",
    "cer_training_1 = perceptron_1.calculateCER(T=T_train_changed_1, Y_hat=Y_hat_training_1, n=n_train_1)\n",
    "print(f\"Classification Error Rate (CER) on the training data of Dataset - 1 is: {cer_training_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the test set using the optimum w_vector . Give the classification error of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_1, X_test_1, T_test_1 = perceptron_1.generateTestData(test_data=test_data_1)\n",
    "\n",
    "X_test_augmented_1 = perceptron_1.augmentData(X=X_test_1, n=n_test_1)\n",
    "print(f\"Shape of Augmented X_test_1: {X_test_augmented_1.shape}\")\n",
    "\n",
    "T_test_changed_1 = perceptron_1.changeLabels(T = T_test_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test_1 = perceptron_1.predict(X=X_test_augmented_1, w_optimum=w_optimum_epochs_1)\n",
    "cer_test_1 = perceptron_1.calculateCER(T=T_test_changed_1, Y_hat=Y_hat_test_1, n=n_test_1)\n",
    "print(f\"Classification Error Rate (CER) on the test data of Dataset - 1 is: {cer_test_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot in feature space the training data points, decision boundaries, and decision regions. The decision boundaries and regions should use the final optimum w_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_1.plotDecisionBoundary(np.hstack((X_train_1, T_train_changed_1.reshape(n_train_1, 1))), w_vector=w_optimum_epochs_1, datasetName=\"Training Data - 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (b) Perform the following for Dataset - 2 of Homework -> 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Run the perceptron learning algorithm to find optimum w_vector using Sequential Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_2 = perceptron.Perceptron()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_2, X_train_2, T_train_2 = perceptron_2.generateTrainData(trainData=train_data_2)\n",
    "X_train_augmented_2 = perceptron_2.augmentData(X=X_train_2, n=n_train_2)\n",
    "print(f\"Shape of Augmented X_train_2: {X_train_augmented_1.shape}\")\n",
    "\n",
    "T_train_changed_2 = perceptron_2.changeLabels(T = T_train_2)\n",
    "\n",
    "X_train_shuffled_2, T_train_shuffled_2 = perceptron_2.shuffleData(X=X_train_augmented_2, T=T_train_changed_2.reshape(n_train_2, 1))\n",
    "\n",
    "w_vector_2 = perceptron_2.initializeWeights(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergenceFlag_2, n_epochs_2, n_epochs_arr_2, n_iters_2, n_iters_arr_2, J_History_epochs_2, w_History_epochs_2, J_History_iterations_2, cer_History_epochs_2, cer_History_iterations_2 = perceptron_2.modelTrain_SequentialGD(\n",
    "                                                                                                                                                                n_train = n_train_2, \n",
    "                                                                                                                                                                X_train=X_train_shuffled_2, \n",
    "                                                                                                                                                                T_train=T_train_shuffled_2, \n",
    "                                                                                                                                                                w_vector=w_vector_2, \n",
    "                                                                                                                                                                epochs=100, \n",
    "                                                                                                                                                                learn_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Give the resulting optimum w vector; state whether the algorithm converged (i.1 reached) or halted without convergence (i.2 reached); and give the final criterion function value J(w_optimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_index_epochs_2 = np.argmin(J_History_epochs_2)\n",
    "optimum_index_iterations_2 = np.argmin(cer_History_iterations_2)\n",
    "\n",
    "w_optimum_epochs_2 = w_History_epochs_2[optimum_index_epochs_2]\n",
    "J_optimum_epochs_2 = J_History_epochs_2[optimum_index_epochs_2]\n",
    "J_optimum_iterations_2 = J_History_iterations_2[optimum_index_iterations_2]\n",
    "cer_optimum_epochs_2 = cer_History_epochs_2[optimum_index_epochs_2]\n",
    "cer_optimum_iterations_2 = cer_History_iterations_2[optimum_index_iterations_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convergenceFlag_2:\n",
    "    print(f\"The Perceptron Learning Algorithm got converged after {n_iters_2} iterations and {n_epochs_2} epoch(s) for Training set of Dataset - 2!\")\n",
    "    print(\"DATASET - 2 IS LINEARLY SEPARABLE!!\")\n",
    "else:\n",
    "    print(f\"The Perceptron Learning Algorithm did not converge for Training set of Dataset - 2 and it took {n_iters_2} iterations and {n_epochs_2} epoch(s)!\")\n",
    "\n",
    "print(f\"The optimum value of w_vector for Training Set of Dataset - 2 is: {w_optimum_epochs_2}\")\n",
    "print(f\"The Final Criterion function value for optimum value of w is: {J_optimum_epochs_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Produce a learning curve, which is a plot of the values of the criterion function during the training process. If the training goes for more than 10 epochs, plot the criterion function vs. epochs. If training ends before 10 epochs, plot the criterion function vs. iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_2 <= 10:\n",
    "    perceptron_2.plotCriterionVsIters(n_iters=n_iters_arr_2, J_History_iters=J_History_iterations_2, J_optimum_iters=J_optimum_iterations_2, datasetName=\"Training Data Dataset - 2\")\n",
    "\n",
    "else:\n",
    "    perceptron_2.plotCriterionVsEpochs(n_epochs=n_epochs_arr_2, J_History_epochs=J_History_epochs_2, J_optimum_epochs=J_optimum_epochs_2, datasetName=\"Training Data Dataset - 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Plot of Classification Error Rate (CER) Vs. Number of Epochs (or) Plot of Classification Error Rate (CER) Vs. Number of Iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_2 <= 10:\n",
    "    perceptron_2.plotCERVsIters(n_iters=n_iters_arr_2, cer_History_iters=cer_History_iterations_2, cer_optimum_iters=cer_optimum_iterations_2, datasetName=\"Training Data Dataset - 2\")\n",
    "\n",
    "else:\n",
    "    perceptron_2.plotCriterionVsEpochs(n_epochs=n_epochs_arr_2, cer_History_epochs=cer_History_epochs_2, cer_optimum_epochs=cer_optimum_epochs_2, datasetName=\"Training Data Dataset - 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the training set using the optimum w_vector . Give the classification error of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_training_2 = perceptron_2.predict(X=X_train_augmented_2, w_optimum=w_optimum_epochs_2)\n",
    "cer_training_2 = perceptron_2.calculateCER(T=T_train_changed_2, Y_hat=Y_hat_training_2, n=n_train_2)\n",
    "print(f\"Classification Error Rate (CER) on the training data of Dataset - 2 is: {cer_training_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the test set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_2, X_test_2, T_test_2 = perceptron_2.generateTestData(test_data=test_data_2)\n",
    "\n",
    "X_test_augmented_2 = perceptron_2.augmentData(X=X_test_2, n=n_test_2)\n",
    "print(f\"Shape of Augmented X_test_2: {X_test_augmented_2.shape}\")\n",
    "\n",
    "T_test_changed_2 = perceptron_2.changeLabels(T = T_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test_2 = perceptron_2.predict(X=X_test_augmented_2, w_optimum=w_optimum_epochs_2)\n",
    "cer_test_2 = perceptron_2.calculateCER(T=T_test_changed_2, Y_hat=Y_hat_test_2, n=n_test_2)\n",
    "print(f\"Classification Error Rate (CER) on the test data of Dataset - 2 is: {cer_test_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv] Plot in feature space the training data points, decision boundaries, and decision regions. The decision boundaries and regions should use the final optimum x_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_2.plotDecisionBoundary(np.hstack((X_train_2, T_train_changed_2.reshape(n_train_2, 1))), w_vector=w_optimum_epochs_2, datasetName=\"Training Data - 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (c) Perform the following for Dataset - 3 of Homework -> 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Run the perceptron learning algorithm to find optimum w_vector using Sequential Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_3 = perceptron.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_3, X_train_3, T_train_3 = perceptron_3.generateTrainData(trainData=train_data_3)\n",
    "X_train_augmented_3 = perceptron_3.augmentData(X=X_train_3, n=n_train_3)\n",
    "print(f\"Shape of Augmented X_train_3: {X_train_augmented_3.shape}\")\n",
    "\n",
    "T_train_changed_3 = perceptron_3.changeLabels(T = T_train_3)\n",
    "\n",
    "X_train_shuffled_3, T_train_shuffled_3 = perceptron_3.shuffleData(X=X_train_augmented_3, T=T_train_changed_3.reshape(n_train_3, 1))\n",
    "\n",
    "w_vector_3 = perceptron_3.initializeWeights(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergenceFlag_3, n_epochs_3, n_epochs_arr_3, n_iters_3, n_iters_arr_3, J_History_epochs_3, w_History_epochs_3, J_History_iterations_3, cer_History_epochs_3, cer_History_iterations_3 = perceptron_3.modelTrain_SequentialGD(\n",
    "                                                                                                                                                                n_train = n_train_3, \n",
    "                                                                                                                                                                X_train=X_train_shuffled_3, \n",
    "                                                                                                                                                                T_train=T_train_shuffled_3, \n",
    "                                                                                                                                                                w_vector=w_vector_3, \n",
    "                                                                                                                                                                epochs=100, \n",
    "                                                                                                                                                                learn_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Give the resulting optimum w vector; state whether the algorithm converged (i.1 reached) or halted without convergence (i.2 reached); and give the final criterion function value J(w_optimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_index_epochs_3 = np.argmin(J_History_epochs_3)\n",
    "optimum_index_iterations_3 = np.argmin(cer_History_iterations_3)\n",
    "\n",
    "w_optimum_epochs_3 = w_History_epochs_3[optimum_index_epochs_3]\n",
    "J_optimum_epochs_3 = J_History_epochs_3[optimum_index_epochs_3]\n",
    "J_optimum_iterations_3 = J_History_iterations_3[optimum_index_iterations_3]\n",
    "cer_optimum_epochs_3 = cer_History_epochs_3[optimum_index_epochs_3]\n",
    "cer_optimum_iterations_3 = cer_History_iterations_3[optimum_index_iterations_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convergenceFlag_3:\n",
    "    print(f\"The Perceptron Learning Algorithm got converged after {n_iters_3} iterations and {n_epochs_3} epoch(s) for Training set of Dataset - 3!\")\n",
    "    print(\"DATA IS LINEARLY SEPARABLE!!\")\n",
    "else:\n",
    "    print(f\"The Perceptron Learning Algorithm did not converge for Training set of Dataset - 3 and it took {n_iters_3} iterations and {n_epochs_3} epoch(s)\")\n",
    "\n",
    "print(f\"The optimum value of w_vector for Training Set of Dataset - 3 is: {w_optimum_epochs_3}\")\n",
    "print(f\"The Final Criterion function value for optimum value of w is: {J_optimum_epochs_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Produce a learning curve, which is a plot of the values of the criterion function during the training process. If the training goes for more than 10 epochs, plot the criterion function vs. epochs. If training ends before 10 epochs, plot the criterion function vs. iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_3 <= 10:\n",
    "    perceptron_3.plotCriterionVsIters(n_iters=n_iters_arr_3, J_History_iters=J_History_iterations_3, J_optimum_iters=J_optimum_iterations_3, datasetName=\"Training Data Dataset - 3\")\n",
    "\n",
    "else:\n",
    "    perceptron_3.plotCriterionVsEpochs(n_epochs=n_epochs_arr_3, J_History_epochs=J_History_epochs_3, J_optimum_epochs=J_optimum_epochs_3, datasetName=\"Training Data Dataset - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Plot of Classification Error Rate (CER) Vs. Number of Epochs (or) Plot of Classification Error Rate (CER) Vs. Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_3 <= 10:\n",
    "    perceptron_3.plotCERVsIters(n_iters=n_iters_arr_3, cer_History_iters=cer_History_iterations_3, cer_optimum_iters=cer_optimum_iterations_3, datasetName=\"Training Data Dataset - 3\")\n",
    "\n",
    "else:\n",
    "    perceptron_3.plotCERVsEpochs(n_epochs=n_epochs_arr_3, cer_History_epochs=cer_History_epochs_3, cer_optimum_epochs=cer_optimum_epochs_3, datasetName=\"Training Data Dataset - 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the training set using the optimum w_vector . Give the classification error of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_training_3 = perceptron_3.predict(X=X_train_augmented_3, w_optimum=w_optimum_epochs_3)\n",
    "cer_training_3 = perceptron_3.calculateCER(T=T_train_changed_3, Y_hat=Y_hat_training_3, n=n_train_3)\n",
    "print(f\"Classification Error Rate (CER) on the training data of Dataset - 3 is: {cer_training_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the test set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_3, X_test_3, T_test_3 = perceptron_3.generateTestData(test_data=test_data_3)\n",
    "\n",
    "X_test_augmented_3 = perceptron_3.augmentData(X=X_test_3, n=n_test_3)\n",
    "print(f\"Shape of Augmented X_test_3: {X_test_augmented_3.shape}\")\n",
    "\n",
    "T_test_changed_3 = perceptron_3.changeLabels(T = T_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test_3 = perceptron_3.predict(X=X_test_augmented_3, w_optimum=w_optimum_epochs_3)\n",
    "cer_test_3 = perceptron_3.calculateCER(T=T_test_changed_3, Y_hat=Y_hat_test_3, n=n_test_3)\n",
    "print(f\"Classification Error Rate (CER) on the test data of Dataset - 3 is: {cer_test_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv] Plot in feature space the training data points, decision boundaries, and decision regions. The decision boundaries and regions should use the final optimum x_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_3.plotDecisionBoundary(np.hstack((X_train_3, T_train_changed_3.reshape(n_train_3, 1))), w_vector=w_optimum_epochs_3, datasetName=\"Training Data - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Breast Cancer Classification Problem using Sequential GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Run the perceptron learning algorithm to find optimum w_vector using Sequential Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bc = np.load(\"./HW3_Datasets/breast_cancer_train.npy\")\n",
    "test_data_bc = np.load(\"./HW3_Datasets/breast_cancer_test.npy\")\n",
    "\n",
    "train_data_bc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_bc = perceptron.Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_bc, X_train_bc, T_train_bc = perceptron_bc.generateTrainDataNumpy(trainData=train_data_bc)\n",
    "\n",
    "X_train_L1_norms = np.linalg.norm(X_train_bc, ord=1, axis=0)\n",
    "\n",
    "X_train_bc_normalized = 100 * X_train_bc / X_train_L1_norms\n",
    "\n",
    "X_train_augmented_bc = perceptron_bc.augmentData(X=X_train_bc_normalized, n=n_train_bc)\n",
    "print(f\"Shape of Augmented X_train_bc: {X_train_augmented_bc.shape}\")\n",
    "\n",
    "T_train_changed_bc = perceptron_bc.changeLabels(T = T_train_bc)\n",
    "\n",
    "X_train_shuffled_bc, T_train_shuffled_bc = perceptron_bc.shuffleData(X=X_train_augmented_bc, T=T_train_changed_bc.reshape(n_train_bc, 1))\n",
    "\n",
    "w_vector_bc = perceptron_bc.initializeWeights(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergenceFlag_bc, n_epochs_bc, n_epochs_arr_bc, n_iters_bc, n_iters_arr_bc, J_History_epochs_bc, w_History_epochs_bc, J_History_iterations_bc, cer_History_epochs_bc, cer_History_iterations_bc = perceptron_bc.modelTrain_SequentialGD(\n",
    "                                                                                                                                                                n_train = n_train_bc, \n",
    "                                                                                                                                                                X_train=X_train_shuffled_bc, \n",
    "                                                                                                                                                                T_train=T_train_shuffled_bc, \n",
    "                                                                                                                                                                w_vector=w_vector_bc, \n",
    "                                                                                                                                                                epochs=100, \n",
    "                                                                                                                                                                learn_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i] Give the resulting optimum w vector; state whether the algorithm converged (i.1 reached) or halted without convergence (i.2 reached); and give the final criterion function value J(w_optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_index_epochs_bc = np.argmin(J_History_epochs_bc)\n",
    "optimum_index_iterations_bc = np.argmin(cer_History_iterations_bc)\n",
    "\n",
    "w_optimum_epochs_bc = w_History_epochs_bc[optimum_index_epochs_bc]\n",
    "J_optimum_epochs_bc = J_History_epochs_bc[optimum_index_epochs_bc]\n",
    "J_optimum_iterations_bc = J_History_iterations_bc[optimum_index_iterations_bc]\n",
    "cer_optimum_epochs_bc = cer_History_epochs_bc[optimum_index_epochs_bc]\n",
    "cer_optimum_iterations_bc = cer_History_iterations_bc[optimum_index_iterations_bc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convergenceFlag_bc:\n",
    "    print(f\"The Perceptron Learning Algorithm got converged after {n_iters_bc} iterations and {n_epochs_bc} epoch(s) for Training set of Breast Cancer Dataset!\")\n",
    "    print(\"DATA IS LINEARLY SEPARABLE!!\")\n",
    "else:\n",
    "    print(f\"The Perceptron Learning Algorithm did not converge for Training set of Breast Cancer Dataset and it took {n_iters_bc} iterations and {n_epochs_bc} epoch(s)!\")\n",
    "\n",
    "print(f\"The optimum value of w_vector for Training Set of BC Dataset is: {w_optimum_epochs_bc}\")\n",
    "print(f\"The Final Criterion function value for optimum value of w is: {J_optimum_epochs_bc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Produce a learning curve, which is a plot of the values of the criterion function during the training process. If the training goes for more than 10 epochs, plot the criterion function vs. epochs. If training ends before 10 epochs, plot the criterion function vs. iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_bc <= 10:\n",
    "    perceptron_bc.plotCriterionVsIters(n_iters=n_iters_arr_bc, J_History_iters=J_History_iterations_bc, J_optimum_iters=J_optimum_iterations_bc, datasetName=\"Training Data BC Dataset\")\n",
    "\n",
    "else:\n",
    "    perceptron_bc.plotCriterionVsEpochs(n_epochs=n_epochs_arr_bc, J_History_epochs=J_History_epochs_bc, J_optimum_epochs=J_optimum_epochs_bc, datasetName=\"Training Data BC Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Plot of Classification Error Rate (CER) Vs. Number of Epochs (or) Plot of Classification Error Rate (CER) Vs. Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_bc <= 10:\n",
    "    perceptron_bc.plotCERVsIters(n_iters=n_iters_arr_bc, cer_History_iters=cer_History_iterations_bc, cer_optimum_iters=cer_optimum_iterations_bc, datasetName=\"Training Data BC Dataset\")\n",
    "\n",
    "else:\n",
    "    perceptron_bc.plotCERVsEpochs(n_epochs=n_epochs_arr_bc, cer_History_epochs=cer_History_epochs_bc, cer_optimum_epochs=cer_optimum_epochs_bc, datasetName=\"Training Data BC Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the training set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_training_bc = perceptron_bc.predict(X=X_train_augmented_bc, w_optimum=w_optimum_epochs_bc)\n",
    "cer_training_bc = perceptron_bc.calculateCER(T=T_train_changed_bc, Y_hat=Y_hat_training_bc, n=n_train_bc)\n",
    "print(f\"Classification Error Rate (CER) on the training data of BC Dataset is: {cer_training_bc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the test set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_bc, X_test_bc, T_test_bc = perceptron_bc.generateTestDataNumy(test_data=test_data_bc)\n",
    "\n",
    "X_test_bc_normalized = 100 * X_test_bc / X_train_L1_norms\n",
    "\n",
    "X_test_augmented_bc = perceptron_bc.augmentData(X=X_test_bc_normalized, n=n_test_bc)\n",
    "print(f\"Shape of Augmented X_test_bc: {X_test_augmented_bc.shape}\")\n",
    "\n",
    "T_test_changed_bc = perceptron_bc.changeLabels(T = T_test_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test_bc = perceptron_bc.predict(X=X_test_augmented_bc, w_optimum=w_optimum_epochs_bc)\n",
    "cer_test_bc = perceptron_bc.calculateCER(T=T_test_changed_bc, Y_hat=Y_hat_test_bc, n=n_test_bc)\n",
    "print(f\"Classification Error Rate (CER) on the test data of BC Dataset is: {cer_test_bc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (d) iv] Is the 2-class data linearly separable? Answer yes, no, or don‚Äôt know. Briefly justify your answer.\n",
    "\n",
    "### <strong> <em> We cannot explicitly comment whether the Breast Cancer Dataset is Linearly Separable or not solely based only on the Classification Error Rate or Cost Function J(w). The Classification Error Rate cannot determine if a given dataset is linearly separable or not. </strong></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv] Instead produce a histogram of the distance from the decision boundary ( g(x) / |ùë§|| ). On the same plot, produce a histogram of this quantity for all class 1 training data and another for all class 2 training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_bc.plotHistogram(X=X_train_augmented_bc, T=T_train_changed_bc, w_vector=w_optimum_epochs_bc, n_train=n_train_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (c) Repeat problem 4(d) (i.e., the breast cancer data) using this Logistic regression and compare the results to that obtained with perceptron learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergenceFlag_bc_sp, n_epochs_bc_sp, n_epochs_arr_bc_sp, n_iters_bc_sp, n_iters_arr_bc_sp, J_History_epochs_bc_sp, w_History_epochs_bc_sp, J_History_iterations_bc_sp, cer_History_epochs_bc_sp, cer_History_iterations_bc_sp = perceptron_bc.modelTrain_SequentialGD_softmaxPlus(\n",
    "                                                                                                                                                                n_train = n_train_bc, \n",
    "                                                                                                                                                                X_train=X_train_shuffled_bc, \n",
    "                                                                                                                                                                T_train=T_train_shuffled_bc, \n",
    "                                                                                                                                                                w_vector=w_vector_bc, \n",
    "                                                                                                                                                                epochs=100, \n",
    "                                                                                                                                                                learn_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i] Give the resulting optimum w vector; state whether the algorithm converged (i.1 reached) or halted without convergence (i.2 reached); and give the final criterion function value J(w_optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_index_epochs_bc_sp = np.argmin(J_History_epochs_bc_sp)\n",
    "optimum_index_iterations_bc_sp = np.argmin(cer_History_iterations_bc_sp)\n",
    "\n",
    "w_optimum_epochs_bc_sp = w_History_epochs_bc_sp[optimum_index_epochs_bc_sp]\n",
    "J_optimum_epochs_bc_sp = J_History_epochs_bc_sp[optimum_index_epochs_bc_sp]\n",
    "J_optimum_iterations_bc_sp = J_History_iterations_bc_sp[optimum_index_iterations_bc_sp]\n",
    "cer_optimum_epochs_bc_sp = cer_History_epochs_bc_sp[optimum_index_epochs_bc_sp]\n",
    "cer_optimum_iterations_bc_sp = cer_History_iterations_bc_sp[optimum_index_iterations_bc_sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convergenceFlag_bc_sp:\n",
    "    print(f\"The Perceptron Learning Algorithm got converged after {n_iters_bc_sp} iterations and {n_epochs_bc_sp} epoch(s) for Training set of Breast Cancer Dataset!\")\n",
    "    print(\"DATA IS LINEARLY SEPARABLE!!\")\n",
    "else:\n",
    "    print(f\"The Perceptron Learning Algorithm did not converge for Training set of Breast Cancer Dataset and it took {n_iters_bc_sp} iterations and {n_epochs_bc_sp} epoch(s)!\")\n",
    "\n",
    "print(f\"The optimum value of w_vector for Training Set of BC Dataset is: {w_optimum_epochs_bc_sp}\")\n",
    "print(f\"The Final Criterion function value for optimum value of w is: {J_optimum_epochs_bc_sp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Produce a learning curve, which is a plot of the values of the criterion function during the training process. If the training goes for more than 10 epochs, plot the criterion function vs. epochs. If training ends before 10 epochs, plot the criterion function vs. iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_bc_sp <= 10:\n",
    "    perceptron_bc.plotCriterionVsIters(n_iters=n_iters_arr_bc_sp, J_History_iters=J_History_iterations_bc_sp, J_optimum_iters=J_optimum_iterations_bc_sp, datasetName=\"Training Data BC Dataset SoftPlus\")\n",
    "\n",
    "else:\n",
    "    perceptron_bc.plotCriterionVsEpochs(n_epochs=n_epochs_arr_bc_sp, J_History_epochs=J_History_epochs_bc_sp, J_optimum_epochs=J_optimum_epochs_bc_sp, datasetName=\"Training Data BC Dataset SoftPlus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii] Plot of Classification Error Rate (CER) Vs. Number of Epochs (or) Plot of Classification Error Rate (CER) Vs. Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_bc_sp <= 10:\n",
    "    perceptron_bc.plotCERVsIters(n_iters=n_iters_arr_bc_sp, cer_History_iters=cer_History_iterations_bc_sp, cer_optimum_iters=cer_optimum_iterations_bc_sp, datasetName=\"Training Data BC Dataset SoftPlus\")\n",
    "\n",
    "else:\n",
    "    perceptron_bc.plotCERVsEpochs(n_epochs=n_epochs_arr_bc_sp, cer_History_epochs=cer_History_epochs_bc_sp, cer_optimum_epochs=cer_optimum_epochs_bc_sp, datasetName=\"Training Data BC Dataset SoftPlus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the training set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_training_bc_sp = perceptron_bc.predict(X=X_train_augmented_bc, w_optimum=w_optimum_epochs_bc_sp)\n",
    "cer_training_bc_sp = perceptron_bc.calculateCER(T=T_train_changed_bc, Y_hat=Y_hat_training_bc_sp, n=n_train_bc)\n",
    "print(f\"Classification Error Rate (CER) on the training data of BC Dataset is: {cer_training_bc_sp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii] Run the perceptron classifier on the test set using the optimum w_vector . Give the classification error of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_test_bc_sp = perceptron_bc.predict(X=X_test_augmented_bc, w_optimum=w_optimum_epochs_bc_sp)\n",
    "cer_test_bc_sp = perceptron_bc.calculateCER(T=T_test_changed_bc, Y_hat=Y_hat_test_bc_sp, n=n_test_bc)\n",
    "print(f\"Classification Error Rate (CER) on the test data of BC Dataset is: {cer_test_bc_sp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results to that obtained with perceptron learning.\n",
    "### <strong> <em> The Classification Error Rate for Perceptron Learning on the Training and Test set and the Classification Error rate on the Training and Test set is almost the same not major changes or improvements can be seen. But we can see the Cost Function J vs the # of epochs has become really smooth.</strong> </em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a93cbcc8f689737a56379ee2b682729639c657947f027c47bdf18bc4994fa06e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
