{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework : 5 Machine Learning - 1 (Supervised Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] (a) 2-class MSE Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.linearRegression import Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the train and test datasets using pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./datasets/train.csv\", header=None)\n",
    "print(\"Train Data -> 1: \")\n",
    "print(train_data.head())\n",
    "print()\n",
    "\n",
    "test_data = pd.read_csv(\"./datasets/test.csv\", header=None)\n",
    "print(\"Test Data -> 1: \")\n",
    "print(test_data.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an instance for the Regressor class in utils.LinearRegression library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Regressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) i] Code a MSE classifier with 7 different degrees of polynomial (ùëù = 1,2,3,4,5,6,7). As a baseline, do not use any regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, X_train, T_train = regressor.generateTrainData(trainData=train_data, printFlag=True)\n",
    "n_test, X_test, T_test  = regressor.generateTestData(testData = test_data, printFlag=True)\n",
    "\n",
    "T_train_changed = regressor.changeLabels(T = T_train, n =  n_train)\n",
    "T_test_changed = regressor.changeLabels(T = T_test, n =  n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing non-linear transfromation on the train and test data for p = 1, 2, 3, 4, 5, 6, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p1 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=1, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p1 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=1, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p2 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=2, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p2 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=2, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p3 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=3, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p3 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=3, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p4 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=4, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p4 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=4, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p5 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=5, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p5 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=5, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p6 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=6, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p6 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=6, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train_p7 = regressor.generatePolynomialFeatures(X = X_train, n = n_train, degree=7, datasetName = \"Train Data\", printFlag=True)\n",
    "X_test_p7 = regressor.generatePolynomialFeatures(X = X_test, n = n_test, degree=7, datasetName = \"Test Data\", printFlag=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using sklearn's Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 1\n",
    "model_p1, w_vector_1 = regressor.modelTrain(X_train = X_train_p1, T_train = T_train_changed)\n",
    "\n",
    "# p = 2\n",
    "model_p2, w_vector_2 = regressor.modelTrain(X_train = X_train_p2, T_train = T_train_changed)\n",
    "\n",
    "# p = 3\n",
    "model_p3, w_vector_3 = regressor.modelTrain(X_train = X_train_p3, T_train = T_train_changed)\n",
    "\n",
    "# p = 4\n",
    "model_p4, w_vector_4 = regressor.modelTrain(X_train = X_train_p4, T_train = T_train_changed)\n",
    "\n",
    "# p = 5\n",
    "model_p5, w_vector_5 = regressor.modelTrain(X_train = X_train_p5, T_train = T_train_changed)\n",
    "\n",
    "# p = 6\n",
    "model_p6, w_vector_6 = regressor.modelTrain(X_train = X_train_p6, T_train = T_train_changed)\n",
    "\n",
    "# p = 7\n",
    "model_p7, w_vector_7 = regressor.modelTrain(X_train = X_train_p7, T_train = T_train_changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] i. Report the classification accuracy on the training and test sets for each value of p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p1, accuracy_train_p1 = regressor.predictTrain(X=X_train_p1, T=T_train_changed, n = n_train, p = 1, model=model_p1, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p1, accuracy_test_p1 = regressor.predictTrain(X=X_test_p1, T=T_test_changed, n = n_test, p = 1, model=model_p1, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p2, accuracy_train_p2 = regressor.predictTrain(X=X_train_p2, T=T_train_changed, n = n_train, p = 2, model=model_p2, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p2, accuracy_test_p2 = regressor.predictTrain(X=X_test_p2, T=T_test_changed, n = n_test, p = 2, model=model_p2, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p3, accuracy_train_p3 = regressor.predictTrain(X=X_train_p3, T=T_train_changed, n = n_train, p = 3, model=model_p3, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p3, accuracy_test_p3 = regressor.predictTrain(X=X_test_p3, T=T_test_changed, n = n_test, p = 3, model=model_p3, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p4, accuracy_train_p4 = regressor.predictTrain(X=X_train_p4, T=T_train_changed, n = n_train, p = 4, model=model_p4, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p4, accuracy_test_p4 = regressor.predictTrain(X=X_test_p4, T=T_test_changed, n = n_test, p = 4, model=model_p4, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p5, accuracy_train_p5 = regressor.predictTrain(X=X_train_p5, T=T_train_changed, n = n_train, p = 5, model=model_p5, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p5, accuracy_test_p5 = regressor.predictTrain(X=X_test_p5, T=T_test_changed, n = n_test, p = 5, model=model_p5, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p6, accuracy_train_p6 = regressor.predictTrain(X=X_train_p6, T=T_train_changed, n = n_train, p = 6, model=model_p6, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p6, accuracy_test_p6 = regressor.predictTrain(X=X_test_p6, T=T_test_changed, n = n_test, p = 6, model=model_p6, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_p7, accuracy_train_p7 = regressor.predictTrain(X=X_train_p7, T=T_train_changed, n = n_train, p = 7, model=model_p7, datasetName = \"Train Data\", printFlag = True)\n",
    "y_hat_test_p7, accuracy_test_p7 = regressor.predictTrain(X=X_test_p7, T=T_test_changed, n = n_test, p = 7, model=model_p7, datasetName = \"Test Data\", printFlag = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] ii. Plot the training data points and the decision regions for p = 1,2,4,7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 1\n",
    "print(\"Plot for p = 1\")\n",
    "regressor.plotNonLinear(X_train=X_train, T_train = T_train.reshape(n_train,), degree=1, model=model_p1)\n",
    "\n",
    "#p = 2\n",
    "print(\"Plot for p = 2\")\n",
    "regressor.plotNonLinear(X_train=X_train, T_train = T_train.reshape(n_train,), degree=2, model=model_p2)\n",
    "\n",
    "#p = 4\n",
    "print(\"Plot for p = 4\")\n",
    "regressor.plotNonLinear(X_train=X_train, T_train = T_train.reshape(n_train,), degree=4, model=model_p4)\n",
    "\n",
    "#p = 7\n",
    "print(\"Plot for p = 7\")\n",
    "regressor.plotNonLinear(X_train=X_train, T_train = T_train.reshape(n_train,), degree=7, model=model_p7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] iii. Plot the train and test accuracy vs. p on a single plot for all values of p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Acc_Hist = [accuracy_train_p1, accuracy_train_p2, accuracy_train_p3, accuracy_train_p4, accuracy_train_p5, accuracy_train_p6, accuracy_train_p7]\n",
    "test_Acc_Hist = [accuracy_test_p1, accuracy_test_p2, accuracy_test_p3, accuracy_test_p4, accuracy_test_p5, accuracy_test_p6, accuracy_test_p7]\n",
    "\n",
    "regressor.plotAccuracy(train_Accuracy_History=train_Acc_Hist, test_Accuracy_History=test_Acc_Hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] iv. Plot JMSE vs. p for all values of p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_History = []\n",
    "J_MSE_1 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p1)\n",
    "J_History.append(J_MSE_1)\n",
    "\n",
    "J_MSE_2 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p2)\n",
    "J_History.append(J_MSE_2)\n",
    "\n",
    "J_MSE_3 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p3)\n",
    "J_History.append(J_MSE_3)\n",
    "\n",
    "J_MSE_4 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p4)\n",
    "J_History.append(J_MSE_4)\n",
    "\n",
    "J_MSE_5 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p5)\n",
    "J_History.append(J_MSE_5)\n",
    "\n",
    "J_MSE_6 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p6)\n",
    "J_History.append(J_MSE_6)\n",
    "\n",
    "J_MSE_7 = regressor.computeCost(T_train=T_train_changed, Y_hat=y_hat_train_p7)\n",
    "J_History.append(J_MSE_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.plotCost(J_History=J_History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Compare and comment on the results in part (a). In particular, how does the train and test accuracy vary as p increases. Also, how do the decision boundaries appear? Do you see any effect of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Accuracy:\n",
    "\n",
    "As we increase the degree p from 1 to 7, There is a very good improvement in the Training Accuracy because as p increases we tend to overfit the data to the training data and there is good improvement in Accuracy.\n",
    "\n",
    "### Test Accuracy:\n",
    "\n",
    "As we increase the degree p from 1 to 2, There is a very good improvement in the Test Accuracy because there is a good fit on the training data. Once we increase the number of dimensions from p = 3 to p = 7 we can clearly see the Test Accuracy drops down and becomes very less when p becomes 7. As already mentioned we had overfit the data and the performance on Test Set has been affected.\n",
    "\n",
    "### Decision Boundary:\n",
    "\n",
    "The Decision Boundary is Non-Linear on the original two-dimensional feature space. The data is clearly linearly un-separable. Once we move to p = 7, we can clearly see how the decision boundary became inclined towards the Training Data due to over-fitting.\n",
    "\n",
    "Yes, I can clearly see the problem of overfitting as the number of dimensions increase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) How many d.o.f and constraints are there (for each p) and how do they relate to the obtained results?\n",
    "\n",
    "### p = 1 ------->  d.o.f = 3, constraints = 60\n",
    "\n",
    "\n",
    "### p = 2 ------->  d.o.f = 6, constraints = 60\n",
    "\n",
    "\n",
    "### p = 3 ------->  d.o.f = 10, constraints = 60\n",
    "\n",
    "\n",
    "### p = 4 ------->  d.o.f = 15, constraints = 60\n",
    "\n",
    "\n",
    "### p = 5 ------->  d.o.f = 21, constraints = 60\n",
    "\n",
    "\n",
    "### p = 6 ------->  d.o.f = 28, constraints = 60\n",
    "\n",
    "\n",
    "### p = 7 ------->  d.o.f = 36, constraints = 60\n",
    "\n",
    "\n",
    "Its very evident from the plots and Accuracy values, when constraints > 10 times d.o.f as in case of p = 2, the model has a good fit on the Training data and performs really well on the Test Data.\n",
    "\n",
    "Its clear that when the number of dimensions, d.o.f is very small as in case of p = 1, here the training and test accuracy both are less and we are kind of doing a underfit on the training data and it affects the performance.\n",
    "\n",
    "Its also very candid that when p becomes very large like 7, the d.o.f becomes 36 and its almost more than half of the number of constraints, in this case the model is overfitting on the training data and performs very well on training data but its accuracy very poor on the Test data and it is lesser than that for p = 1.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
